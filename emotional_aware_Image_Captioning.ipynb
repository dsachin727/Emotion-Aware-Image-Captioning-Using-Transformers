{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kg5j3t7p48Xe"
      },
      "outputs": [],
      "source": [
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import random\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, get_scheduler, GPT2Config\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from tqdm import tqdm\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "import re # For clean_caption\n",
        "from sklearn.model_selection import train_test_split # For proper data splitting\n",
        "\n",
        "# --- 0. Configuration and Paths ---\n",
        "# Adjust these paths to where you saved your models and where Flickr30k is located\n",
        "\n",
        "# FLICKR_BASE_PATH should be the directory that DIRECTLY contains the image files (e.g., 1000092795.jpg)\n",
        "# If your images are in /content/MyDrive/data/flickr30k_images/, set it to that.\n",
        "# Based on your previous successful runs, it seems your images are in the innermost 'flickr30k_images' folder.\n",
        "FLICKR_BASE_PATH = '/content/MyDrive/MyDrive/M.Sc Data Science/Sem 4/CV/flickr30k_images/flickr30k_images/flickr30k_images'\n",
        "\n",
        "CAPTIONS_FILE = '/content/MyDrive/MyDrive/M.Sc Data Science/Sem 4/CV/flickr30k_images/flickr_captions.json'\n",
        "EMOTION_MODEL_PATH = '/content/MyDrive/MyDrive/M.Sc Data Science/Sem 4/CV/eff_fine_tuned_model.pth' # Path to your saved emotion model\n",
        "# This path should be a specific file for saving the best trained combined model\n",
        "CAPTIONING_MODEL_SAVE_PATH = '/content/MyDrive/MyDrive/M.Sc Data Science/Sem 4/CV/fine_tuned_best_model/best_caption_model_finetuned_emotion_aware.pt'\n",
        "\n",
        "# Ensure the directory for saving the model exists\n",
        "os.makedirs(os.path.dirname(CAPTIONING_MODEL_SAVE_PATH), exist_ok=True)\n",
        "\n",
        "# --- Path Validation Check for Image Directory ---\n",
        "# This helps ensure the FLICKR_BASE_PATH points to the correct location of images.\n",
        "if not os.path.isdir(FLICKR_BASE_PATH):\n",
        "    print(f\"Error: FLICKR_BASE_PATH '{FLICKR_BASE_PATH}' does not exist or is not a directory.\")\n",
        "    print(\"Please verify the path to your Flickr30k image folder in Google Drive.\")\n",
        "    # Consider exiting here if the path is critical: sys.exit(1)\n",
        "elif not any(fname.lower().endswith(('.jpg', '.jpeg', '.png')) for fname in os.listdir(FLICKR_BASE_PATH)):\n",
        "    print(f\"Warning: FLICKR_BASE_PATH '{FLICKR_BASE_PATH}' does not appear to contain image files directly.\")\n",
        "    print(\"This might mean you need to adjust the path to the actual image subdirectory (e.g., 'path/to/flickr30k_images/flickr30k_images' if it's nested).\")\n",
        "    print(\"The script will attempt to proceed, but you might encounter many 'Image not found' fallbacks.\")\n",
        "else:\n",
        "    print(f\"FLICKR_BASE_PATH '{FLICKR_BASE_PATH}' seems valid and contains images.\")\n",
        "\n",
        "# Hyperparameters for full training\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 16\n",
        "NUM_CLASSES = 7 # Adjust according to your emotion dataset (e.g., 7 emotions)\n",
        "MAX_CAPTION_LENGTH = 25\n",
        "NUM_EPOCHS = 1 # Increased epochs for better training\n",
        "LEARNING_RATE = 5e-5\n",
        "PATIENCE = 5 # For EarlyStopping\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# --- 1. Utility Functions ---\n",
        "\n",
        "# Function to clean captions (copied from your Image captioning model code.pdf)\n",
        "def clean_caption(text):\n",
        "    text = re.sub(r'([.!?]{1,3}){2,}', \".\", text) # Replace multiple punctuation with single dot\n",
        "    sentence_end = re.search(r'[.!?]', text)\n",
        "    if sentence_end:\n",
        "        text = text[:sentence_end.end()]\n",
        "    text = text.strip(\"`.'\")\n",
        "    if text:\n",
        "        text = text[0].upper() + text[1:] if len(text) > 1 else text.upper()\n",
        "    return text.lower() # Return lower case for BLEU calculation\n",
        "\n",
        "# EarlyStopping class (copied and adapted from your emotion model code.pdf)\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, verbose=False, delta=0):\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = float('inf')\n",
        "        self.delta = delta\n",
        "        self.best_state = None\n",
        "\n",
        "    def __call__(self, val_loss, model_state_dict):\n",
        "        score = -val_loss # We want to minimize loss, so maximize -loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.val_loss_min = val_loss\n",
        "            self.best_state = model_state_dict\n",
        "            if self.verbose:\n",
        "                print(f\"Validation loss decreased ({self.val_loss_min:.4f} --> {val_loss:.4f}). Saving model ...\")\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            if self.verbose:\n",
        "                print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.val_loss_min = val_loss\n",
        "            self.best_state = model_state_dict\n",
        "            self.counter = 0\n",
        "            if self.verbose:\n",
        "                print(f\"Validation loss decreased ({self.val_loss_min:.4f} --> {val_loss:.4f}). Saving model ...\")\n",
        "\n",
        "\n",
        "# --- 2. Data Preparation ---\n",
        "\n",
        "# Image Transforms for all models\n",
        "image_transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Custom Dataset Class (from your Image captioning model code.pdf)\n",
        "class ImageCaptionDataset(Dataset):\n",
        "    def __init__(self, image_folder, samples, tokenizer, transform):\n",
        "        self.image_folder = image_folder\n",
        "        self.samples = samples # list of (img_name, caption)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.transform = transform\n",
        "        self.max_length = MAX_CAPTION_LENGTH # Use global max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name, caption = self.samples[idx]\n",
        "        img_path = os.path.join(self.image_folder, img_name)\n",
        "\n",
        "        try:\n",
        "            image = Image.open(img_path).convert(\"RGB\")\n",
        "        except (OSError, UnidentifiedImageError):\n",
        "            # Fallback to a black image if unreadable\n",
        "            image = Image.new('RGB', (IMG_SIZE, IMG_SIZE), (0, 0, 0))\n",
        "\n",
        "        image = self.transform(image)\n",
        "        tokens = self.tokenizer(caption, return_tensors='pt', padding='max_length',\n",
        "                                truncation=True, max_length=self.max_length)\n",
        "        input_ids = tokens['input_ids'].squeeze(0)\n",
        "        attention_mask = tokens['attention_mask'].squeeze(0)\n",
        "\n",
        "        # Return all five items as before. The loop will decide which ones to unpack.\n",
        "        return image, input_ids, attention_mask, caption, img_name\n",
        "\n",
        "# Load captions (Flickr30k)\n",
        "with open(CAPTIONS_FILE, 'r') as f:\n",
        "    captions_data = json.load(f)\n",
        "\n",
        "# Prepare all samples (img_name, caption) - using first 5 captions for each image\n",
        "all_image_caption_pairs = []\n",
        "for img_name, caps in captions_data.items():\n",
        "    for cap in caps:\n",
        "        all_image_caption_pairs.append((img_name, cap))\n",
        "\n",
        "print(f\"Total image-caption pairs loaded: {len(all_image_caption_pairs)}\")\n",
        "\n",
        "# Create tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "tokenizer.pad_token = tokenizer.eos_token # pad token set to eos token\n",
        "tokenizer.bos_token = tokenizer.bos_token or tokenizer.eos_token\n",
        "\n",
        "# --- Proper Train/Validation/Test Split ---\n",
        "# We'll split the unique images first, then distribute their captions\n",
        "unique_image_names = list(captions_data.keys())\n",
        "train_img_names, test_img_names = train_test_split(unique_image_names, test_size=0.1, random_state=42)\n",
        "train_img_names, val_img_names = train_test_split(train_img_names, test_size=0.1/0.9, random_state=42) # 10% of total for val\n",
        "\n",
        "train_samples = [(img, cap) for img, cap in all_image_caption_pairs if img in train_img_names]\n",
        "val_samples = [(img, cap) for img, cap in all_image_caption_pairs if img in val_img_names]\n",
        "test_samples = [(img, cap) for img, cap in all_image_caption_pairs if img in test_img_names]\n",
        "\n",
        "print(f\"Train samples: {len(train_samples)}\")\n",
        "print(f\"Validation samples: {len(val_samples)}\")\n",
        "print(f\"Test samples: {len(test_samples)}\")\n",
        "\n",
        "train_dataset = ImageCaptionDataset(FLICKR_BASE_PATH, train_samples, tokenizer, image_transform)\n",
        "val_dataset = ImageCaptionDataset(FLICKR_BASE_PATH, val_samples, tokenizer, image_transform)\n",
        "test_dataset = ImageCaptionDataset(FLICKR_BASE_PATH, test_samples, tokenizer, image_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=16) # Increased workers\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=16)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=16)\n",
        "\n",
        "\n",
        "# --- 3. Model Definitions ---\n",
        "\n",
        "# 3.1. Emotion Classification Model (Copied from your emotion model code)\n",
        "def create_emotion_model(base_model_name='efficientnet_b0', num_classes=NUM_CLASSES, pretrained=True):\n",
        "    if base_model_name == 'efficientnet_b0':\n",
        "        model = models.efficientnet_b0(pretrained=pretrained)\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False # Freeze base layers\n",
        "        in_features = model.classifier[1].in_features\n",
        "        model.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(in_features, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, num_classes),\n",
        "        )\n",
        "    elif base_model_name == 'resnet50':\n",
        "        model = models.resnet50(pretrained=pretrained)\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False # Freeze base layers\n",
        "        in_features = model.fc.in_features\n",
        "        model.fc = nn.Sequential(\n",
        "            nn.Linear(in_features, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, num_classes),\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported model name\")\n",
        "    return model.to(device)\n",
        "\n",
        "# Load the trained emotion classification model\n",
        "emotion_classifier = create_emotion_model('efficientnet_b0', NUM_CLASSES, pretrained=False)\n",
        "emotion_classifier.load_state_dict(torch.load(EMOTION_MODEL_PATH, map_location=device))\n",
        "emotion_classifier.eval() # Set to evaluation mode; weights are frozen\n",
        "print(f\"Loaded emotion classification model from {EMOTION_MODEL_PATH}\")\n",
        "\n",
        "# 3.2. Core Captioning Model Components (from your image captioning model code)\n",
        "encoder_cnn = models.efficientnet_b0(pretrained=True)\n",
        "encoder_cnn.classifier = nn.Identity() # Remove classifier head\n",
        "encoder_cnn.to(device)\n",
        "encoder_cnn.eval() # Freeze encoder; its weights are not updated\n",
        "\n",
        "decoder = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "decoder.resize_token_embeddings(len(tokenizer))\n",
        "decoder.to(device)\n",
        "\n",
        "projector = nn.Linear(1280, decoder.config.n_embd).to(device) # Project encoder features to GPT-2 embedding dim\n",
        "\n",
        "# --- 4. Emotion-Aware Model Definition (NEW COMBINED MODEL) ---\n",
        "\n",
        "class EmotionAwareCaptioningModel(nn.Module):\n",
        "    def __init__(self, encoder_cnn, emotion_classifier, projector, decoder, num_emotions, decoder_embedding_dim):\n",
        "        super().__init__()\n",
        "        self.encoder_cnn = encoder_cnn\n",
        "        self.emotion_classifier = emotion_classifier\n",
        "        self.projector = projector\n",
        "        self.decoder = decoder\n",
        "        # A linear layer to project the 7-dim emotion probabilities into the decoder's embedding dimension\n",
        "        self.emotion_projection = nn.Linear(num_emotions, decoder_embedding_dim).to(device)\n",
        "\n",
        "    def forward(self, images, input_ids=None, attention_mask=None):\n",
        "        # 1. Get visual features from CNN encoder\n",
        "        with torch.no_grad(): # Ensure encoder_cnn and emotion_classifier remain frozen\n",
        "            image_features = self.encoder_cnn(images) # (batch_size, 1280)\n",
        "            image_embeddings = self.projector(image_features).unsqueeze(1) # (batch_size, 1, decoder_embedding_dim)\n",
        "\n",
        "            # 2. Get emotion prediction from emotion classifier\n",
        "            emotion_logits = self.emotion_classifier(images) # (batch_size, NUM_CLASSES)\n",
        "            emotion_probs = torch.softmax(emotion_logits, dim=-1) # (batch_size, NUM_CLASSES)\n",
        "            # Project emotion probabilities to the decoder's embedding dimension\n",
        "            emotion_embedding = self.emotion_projection(emotion_probs).unsqueeze(1) # (batch_size, 1, decoder_embedding_dim)\n",
        "\n",
        "        # 3. Get embeddings for captions (for teacher forcing during training)\n",
        "        if input_ids is not None:\n",
        "            # Shift input_ids for teacher forcing, add BOS token explicitly\n",
        "            bos_tokens = torch.full((input_ids.shape[0], 1), tokenizer.bos_token_id, dtype=torch.long, device=device)\n",
        "            # Concatenate BOS token to the beginning of input_ids\n",
        "            decoder_input_ids = torch.cat([bos_tokens, input_ids], dim=1)\n",
        "            caption_embeddings = self.decoder.transformer.wte(decoder_input_ids) # (batch_size, seq_len+1, embd_dim)\n",
        "\n",
        "            # 4. Concatenate image, emotion, and caption embeddings\n",
        "            # The order here is crucial: [image_embedding, emotion_embedding, caption_embeddings]\n",
        "            # This makes the first effective input to the decoder a combined visual+emotional token.\n",
        "            decoder_inputs = torch.cat([image_embeddings, emotion_embedding, caption_embeddings[:, :-1]], dim=1) # Remove last token from caption_embeddings for shifted input\n",
        "        else:\n",
        "            # For inference, only image_embedding and emotion_embedding form the initial input\n",
        "            decoder_inputs = torch.cat([image_embeddings, emotion_embedding], dim=1)\n",
        "\n",
        "        # 5. Pass through GPT-2 decoder\n",
        "        if input_ids is not None:\n",
        "            # For training, we provide the whole sequence of embeddings\n",
        "            # Corrected: expanded_attention_mask should match the length of decoder_inputs\n",
        "            expanded_attention_mask = torch.cat([\n",
        "                torch.ones(images.shape[0], 2, dtype=torch.long, device=device), # For image_embed and emotion_embed (2 tokens)\n",
        "                attention_mask # Use the full original attention_mask for the caption part (MAX_CAPTION_LENGTH tokens)\n",
        "            ], dim=1)\n",
        "            # This results in an attention mask length of 2 + MAX_CAPTION_LENGTH, which matches decoder_inputs.\n",
        "\n",
        "            outputs = self.decoder(inputs_embeds=decoder_inputs, attention_mask=expanded_attention_mask)\n",
        "            return outputs\n",
        "        else:\n",
        "            # For inference, the generate method handles sequence generation\n",
        "            # Initial inputs_embeds for generation\n",
        "            initial_attention_mask = torch.ones(decoder_inputs.shape[0], decoder_inputs.shape[1], dtype=torch.long, device=device)\n",
        "            generated_output = self.decoder.generate(\n",
        "                inputs_embeds=decoder_inputs,\n",
        "                max_length=MAX_CAPTION_LENGTH,\n",
        "                num_beams=3, # Use beam search for better quality\n",
        "                early_stopping=True,\n",
        "                no_repeat_ngram_size=2,\n",
        "                pad_token_id=tokenizer.pad_token_id,\n",
        "                eos_token_id=tokenizer.eos_token_id,\n",
        "                attention_mask=initial_attention_mask # Initial attention mask for the prompt\n",
        "            )\n",
        "            return generated_output\n",
        "\n",
        "# Instantiate the full emotion-aware model\n",
        "emotion_aware_model = EmotionAwareCaptioningModel(\n",
        "    encoder_cnn=encoder_cnn,\n",
        "    emotion_classifier=emotion_classifier,\n",
        "    projector=projector,\n",
        "    decoder=decoder,\n",
        "    num_emotions=NUM_CLASSES,\n",
        "    decoder_embedding_dim=decoder.config.n_embd\n",
        ")\n",
        "emotion_aware_model.to(device)\n",
        "\n",
        "# --- 5. Training Configuration for Emotion-Aware Model ---\n",
        "\n",
        "# Only optimize parameters of the projector, decoder, and the new emotion_projection\n",
        "optimizer = optim.AdamW(\n",
        "    list(projector.parameters()) +\n",
        "    list(decoder.parameters()) +\n",
        "    list(emotion_aware_model.emotion_projection.parameters()),\n",
        "    lr=LEARNING_RATE\n",
        ")\n",
        "\n",
        "# CrossEntropyLoss expects logits and target indices (not one-hot)\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "\n",
        "# --- 6. Training Loop ---\n",
        "early_stopping = EarlyStopping(patience=PATIENCE, verbose=True)\n",
        "\n",
        "print(\"\\n--- Starting Emotion-Aware Captioning Model Training ---\")\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    emotion_aware_model.train() # Set model to training mode\n",
        "    total_train_loss = 0.0\n",
        "\n",
        "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} - Training\")\n",
        "    # Corrected unpacking: Expect 5 items from the dataset, but only unpack the first 3 for training\n",
        "    for images, input_ids, attention_mask, _, _ in loop:\n",
        "        images, input_ids, attention_mask = images.to(device), input_ids.to(device), attention_mask.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass through the emotion-aware model\n",
        "        outputs = emotion_aware_model(images, input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        # Logits for the actual caption tokens start from index 2\n",
        "        # (index 0 is image_embed, index 1 is emotion_embed)\n",
        "        logits_for_caption = outputs.logits[:, 2:, :]\n",
        "\n",
        "        # Labels are the actual input_ids (from tokenizer), reshaped\n",
        "        loss = loss_fn(logits_for_caption.reshape(-1, logits_for_caption.size(-1)), input_ids.reshape(-1))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        loop.set_postfix(loss=loss.item())\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1} Train Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # --- Validation Loop ---\n",
        "    emotion_aware_model.eval() # Set model to evaluation mode\n",
        "    total_val_loss = 0.0\n",
        "    val_loop = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} - Validation\")\n",
        "    with torch.no_grad():\n",
        "        # Corrected unpacking for validation loop as well\n",
        "        for images, input_ids, attention_mask, _, _ in val_loop:\n",
        "            images, input_ids, attention_mask = images.to(device), input_ids.to(device), attention_mask.to(device)\n",
        "\n",
        "            outputs = emotion_aware_model(images, input_ids=input_ids, attention_mask=attention_mask)\n",
        "            logits_for_caption = outputs.logits[:, 2:, :]\n",
        "            loss = loss_fn(logits_for_caption.reshape(-1, logits_for_caption.size(-1)), input_ids.reshape(-1))\n",
        "            total_val_loss += loss.item()\n",
        "            val_loop.set_postfix(val_loss=loss.item())\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(val_loader)\n",
        "    print(f\"Epoch {epoch+1} Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    # --- Early Stopping and Model Saving ---\n",
        "    # Prepare model state for saving\n",
        "    model_state = {\n",
        "        'encoder_cnn_state_dict': encoder_cnn.state_dict(), # Not strictly necessary if always frozen, but good for completeness\n",
        "        'projector_state_dict': projector.state_dict(),\n",
        "        'decoder_state_dict': decoder.state_dict(),\n",
        "        'emotion_projection_state_dict': emotion_aware_model.emotion_projection.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'tokenizer': tokenizer,\n",
        "        'epoch': epoch + 1,\n",
        "        'val_loss': avg_val_loss\n",
        "    }\n",
        "\n",
        "    early_stopping(avg_val_loss, model_state)\n",
        "    if early_stopping.early_stop:\n",
        "        print(\"Early stopping triggered. Restoring best model weights.\")\n",
        "        # Load the best state back if early stopping is triggered\n",
        "        best_state = early_stopping.best_state\n",
        "        projector.load_state_dict(best_state['projector_state_dict'])\n",
        "        decoder.load_state_dict(best_state['decoder_state_dict'])\n",
        "        emotion_aware_model.emotion_projection.load_state_dict(best_state['emotion_projection_state_dict'])\n",
        "        break # Exit training loop\n",
        "\n",
        "# Save the final (or best) model\n",
        "print(f\"Saving best model to {CAPTIONING_MODEL_SAVE_PATH}\")\n",
        "torch.save(early_stopping.best_state, CAPTIONING_MODEL_SAVE_PATH)\n",
        "\n",
        "\n",
        "# --- 7. Final Inference/Testing Loop ---\n",
        "print(\"\\n--- Running Final Evaluation on Test Set ---\")\n",
        "# Load the best model to ensure evaluation is on the best weights\n",
        "best_saved_state = torch.load(CAPTIONING_MODEL_SAVE_PATH, map_location=device)\n",
        "projector.load_state_dict(best_saved_state['projector_state_dict'])\n",
        "decoder.load_state_dict(best_saved_state['decoder_state_dict'])\n",
        "emotion_aware_model.emotion_projection.load_state_dict(best_saved_state['emotion_projection_state_dict'])\n",
        "\n",
        "emotion_aware_model.eval() # Set model to evaluation mode\n",
        "\n",
        "# --- BLEU Evaluation on Test Set ---\n",
        "hypotheses = [] # List of generated captions (tokenized)\n",
        "references = {} # Dictionary mapping image_name to list of reference captions (tokenized)\n",
        "\n",
        "# Prepare references from full captions_data (ensure test set images have all their references)\n",
        "for img_name, caps in captions_data.items():\n",
        "    # Only include images that are actually in the test_samples for consistency\n",
        "    if img_name in [s[0] for s in test_samples]:\n",
        "        references[img_name] = [clean_caption(cap).lower().split() for cap in caps]\n",
        "\n",
        "# We need to ensure we only generate one caption per unique test image\n",
        "# and that the references are correctly collected for those specific unique images.\n",
        "# The `test_loader` iterates through all samples, which includes 5 captions per image.\n",
        "# We should iterate through unique test image names for caption generation and BLEU calculation.\n",
        "unique_test_image_names = list(set([s[0] for s in test_samples]))\n",
        "\n",
        "hypotheses_for_bleu = []\n",
        "references_for_bleu = []\n",
        "\n",
        "print(\"\\n--- Generating captions for unique test images for BLEU ---\")\n",
        "for img_name in tqdm(unique_test_image_names, desc=\"Generating captions\"):\n",
        "    img_path = os.path.join(FLICKR_BASE_PATH, img_name)\n",
        "    try:\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "    except (OSError, UnidentifiedImageError):\n",
        "        image = Image.new('RGB', (IMG_SIZE, IMG_SIZE), (0, 0, 0)) # Fallback\n",
        "\n",
        "    image_tensor = image_transform(image).unsqueeze(0).to(device) # Add batch dimension\n",
        "\n",
        "    with torch.no_grad():\n",
        "        generated_ids = emotion_aware_model(image_tensor)\n",
        "        pred_caption = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "        pred_caption = clean_caption(pred_caption)\n",
        "\n",
        "    hypotheses_for_bleu.append(pred_caption.split())\n",
        "    # Add all 5 references for this unique image\n",
        "    references_for_bleu.append(references[img_name])\n",
        "\n",
        "\n",
        "# Calculate BLEU scores\n",
        "bleu_1 = corpus_bleu(references_for_bleu, hypotheses_for_bleu, weights=(1, 0, 0, 0))\n",
        "bleu_2 = corpus_bleu(references_for_bleu, hypotheses_for_bleu, weights=(0.5, 0.5, 0, 0))\n",
        "bleu_3 = corpus_bleu(references_for_bleu, hypotheses_for_bleu, weights=(0.33, 0.33, 0.33, 0))\n",
        "bleu_4 = corpus_bleu(references_for_bleu, hypotheses_for_bleu, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "\n",
        "print(f\"\\nFinal BLEU-1 Score on {len(unique_test_image_names)} images: {bleu_1 * 100:.2f}%\")\n",
        "print(f\"Final BLEU-2 Score on {len(unique_test_image_names)} images: {bleu_2 * 100:.2f}%\")\n",
        "print(f\"Final BLEU-3 Score on {len(unique_test_image_names)} images: {bleu_3 * 100:.2f}%\")\n",
        "print(f\"Final BLEU-4 Score on {len(unique_test_image_names)} images: {bleu_4 * 100:.2f}%\")\n",
        "\n",
        "\n",
        "# --- Sample Generated Captions from Test Set ---\n",
        "print(\"\\n--- Example Generated Captions from Test Set (with Predicted Emotion) ---\")\n",
        "# Get a few sample images from the unique test set for qualitative review\n",
        "sample_img_names_for_qualitative = random.sample(unique_test_image_names, min(5, len(unique_test_image_names)))\n",
        "\n",
        "for i, img_name in enumerate(sample_img_names_for_qualitative):\n",
        "    img_path = os.path.join(FLICKR_BASE_PATH, img_name)\n",
        "    try:\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "    except (OSError, UnidentifiedImageError):\n",
        "        image = Image.new('RGB', (IMG_SIZE, IMG_SIZE), (0, 0, 0)) # Fallback\n",
        "\n",
        "    image_tensor = image_transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "    # Predict emotion for the image (using the frozen classifier)\n",
        "    with torch.no_grad():\n",
        "        emotion_logits_pred = emotion_classifier(image_tensor)\n",
        "        predicted_emotion_idx = torch.argmax(emotion_logits_pred, dim=1).item()\n",
        "        emotion_class_names = ['Angry', 'Disgusted', 'Fearful', 'Happy', 'Neutral', 'Sad', 'Surprised']\n",
        "        predicted_emotion_name = emotion_class_names[predicted_emotion_idx]\n",
        "\n",
        "    # Generate caption from the emotion-aware model\n",
        "    with torch.no_grad():\n",
        "        generated_ids = emotion_aware_model(image_tensor)\n",
        "        pred_caption = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "        pred_caption = clean_caption(pred_caption)\n",
        "\n",
        "    print(f\"\\nImage: {img_name}\")\n",
        "    print(f\"Predicted Emotion: {predicted_emotion_name}\")\n",
        "    print(f\"Generated Caption: {pred_caption}\")\n",
        "    print(\"References:\")\n",
        "    for ref_cap in references[img_name]: # Use references dict\n",
        "        print(f\"  - {' '.join(ref_cap)}\") # Join tokens back for display\n",
        "\n",
        "print(\"\\n--- Training and Evaluation Complete ---\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 7. Final Inference/Testing Loop ---\n",
        "print(\"\\n--- Running Final Evaluation on Test Set ---\")\n",
        "# Load the best model to ensure evaluation is on the best weights\n",
        "# Corrected: Added map_location and weights_only=False to load the tokenizer\n",
        "best_saved_state = torch.load(CAPTIONING_MODEL_SAVE_PATH, map_location=device, weights_only=False)\n",
        "projector.load_state_dict(best_saved_state['projector_state_dict'])\n",
        "decoder.load_state_dict(best_saved_state['decoder_state_dict'])\n",
        "emotion_aware_model.emotion_projection.load_state_dict(best_saved_state['emotion_projection_state_dict'])\n",
        "\n",
        "emotion_aware_model.eval() # Set model to evaluation mode\n",
        "\n",
        "# Retrieve the tokenizer from the loaded state\n",
        "tokenizer = best_saved_state['tokenizer'] # Set model to evaluation mode\n",
        "\n",
        "# --- BLEU Evaluation on Test Set ---\n",
        "hypotheses = [] # List of generated captions (tokenized)\n",
        "references = {} # Dictionary mapping image_name to list of reference captions (tokenized)\n",
        "\n",
        "# Prepare references from full captions_data (ensure test set images have all their references)\n",
        "for img_name, caps in captions_data.items():\n",
        "    # Only include images that are actually in the test_samples for consistency\n",
        "    if img_name in [s[0] for s in test_samples]:\n",
        "        references[img_name] = [clean_caption(cap).lower().split() for cap in caps]\n",
        "\n",
        "# We need to ensure we only generate one caption per unique test image\n",
        "# and that the references are correctly collected for those specific unique images.\n",
        "# The `test_loader` iterates through all samples, which includes 5 captions per image.\n",
        "# We should iterate through unique test image names for caption generation and BLEU calculation.\n",
        "unique_test_image_names = list(set([s[0] for s in test_samples]))\n",
        "\n",
        "hypotheses_for_bleu = []\n",
        "references_for_bleu = []\n",
        "\n",
        "print(\"\\n--- Generating captions for unique test images for BLEU ---\")\n",
        "for img_name in tqdm(unique_test_image_names, desc=\"Generating captions\"):\n",
        "    img_path = os.path.join(FLICKR_BASE_PATH, img_name)\n",
        "    try:\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "    except (OSError, UnidentifiedImageError):\n",
        "        image = Image.new('RGB', (IMG_SIZE, IMG_SIZE), (0, 0, 0)) # Fallback\n",
        "\n",
        "    image_tensor = image_transform(image).unsqueeze(0).to(device) # Add batch dimension\n",
        "\n",
        "    with torch.no_grad():\n",
        "        generated_ids = emotion_aware_model(image_tensor)\n",
        "        pred_caption = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "        pred_caption = clean_caption(pred_caption)\n",
        "\n",
        "    hypotheses_for_bleu.append(pred_caption.split())\n",
        "    # Add all 5 references for this unique image\n",
        "    references_for_bleu.append(references[img_name])\n",
        "\n",
        "\n",
        "# Calculate BLEU scores\n",
        "bleu_1 = corpus_bleu(references_for_bleu, hypotheses_for_bleu, weights=(1, 0, 0, 0))\n",
        "bleu_2 = corpus_bleu(references_for_bleu, hypotheses_for_bleu, weights=(0.5, 0.5, 0, 0))\n",
        "bleu_3 = corpus_bleu(references_for_bleu, hypotheses_for_bleu, weights=(0.33, 0.33, 0.33, 0))\n",
        "bleu_4 = corpus_bleu(references_for_bleu, hypotheses_for_bleu, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "\n",
        "print(f\"\\nFinal BLEU-1 Score on {len(unique_test_image_names)} images: {bleu_1 * 100:.2f}%\")\n",
        "print(f\"Final BLEU-2 Score on {len(unique_test_image_names)} images: {bleu_2 * 100:.2f}%\")\n",
        "print(f\"Final BLEU-3 Score on {len(unique_test_image_names)} images: {bleu_3 * 100:.2f}%\")\n",
        "print(f\"Final BLEU-4 Score on {len(unique_test_image_names)} images: {bleu_4 * 100:.2f}%\")\n",
        "\n",
        "\n",
        "# --- Sample Generated Captions from Test Set ---\n",
        "print(\"\\n--- Example Generated Captions from Test Set (with Predicted Emotion) ---\")\n",
        "# Get a few sample images from the unique test set for qualitative review\n",
        "sample_img_names_for_qualitative = random.sample(unique_test_image_names, min(5, len(unique_test_image_names)))\n",
        "\n",
        "for i, img_name in enumerate(sample_img_names_for_qualitative):\n",
        "    img_path = os.path.join(FLICKR_BASE_PATH, img_name)\n",
        "    try:\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "    except (OSError, UnidentifiedImageError):\n",
        "        image = Image.new('RGB', (IMG_SIZE, IMG_SIZE), (0, 0, 0)) # Fallback\n",
        "\n",
        "    image_tensor = image_transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "    # Predict emotion for the image (using the frozen classifier)\n",
        "    with torch.no_grad():\n",
        "        emotion_logits_pred = emotion_classifier(image_tensor)\n",
        "        predicted_emotion_idx = torch.argmax(emotion_logits_pred, dim=1).item()\n",
        "        emotion_class_names = ['Angry', 'Disgusted', 'Fearful', 'Happy', 'Neutral', 'Sad', 'Surprised']\n",
        "        predicted_emotion_name = emotion_class_names[predicted_emotion_idx]\n",
        "\n",
        "    # Generate caption from the emotion-aware model\n",
        "    with torch.no_grad():\n",
        "        generated_ids = emotion_aware_model(image_tensor)\n",
        "        pred_caption = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "        pred_caption = clean_caption(pred_caption)\n",
        "\n",
        "    print(f\"\\nImage: {img_name}\")\n",
        "    print(f\"Predicted Emotion: {predicted_emotion_name}\")\n",
        "    print(f\"Generated Caption: {pred_caption}\")\n",
        "    print(\"References:\")\n",
        "    for ref_cap in references[img_name]: # Use references dict\n",
        "        print(f\"  - {' '.join(ref_cap)}\") # Join tokens back for display\n",
        "\n",
        "print(\"\\n--- Training and Evaluation Complete ---\")\n"
      ],
      "metadata": {
        "id": "hBL5G4-35DAZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}