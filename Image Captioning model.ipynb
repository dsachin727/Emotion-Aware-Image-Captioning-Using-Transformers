{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Obto-6vl5Tki"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/MyDrive/')\n",
        "\n",
        "!pip install --upgrade transformers\n",
        "!pip install --upgrade --force-reinstall sympy\n",
        "\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import random\n",
        "import pandas as pd\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, get_scheduler\n",
        "from tqdm import tqdm\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Paths\n",
        "image_folder = '/content/MyDrive/MyDrive/M.Sc Data Science/Sem 4/CV/flickr30k_images/flickr30k_images/flickr30k_images'\n",
        "captions_json = '/content/MyDrive/MyDrive/M.Sc Data Science/Sem 4/CV/flickr30k_images/flickr_captions.json'\n",
        "model_save_path = '/content/MyDrive/MyDrive/M.Sc Data Science/Sem 4/CV/best_full_caption_model'\n",
        "os.makedirs(model_save_path, exist_ok=True)\n",
        "\n",
        "# Hyperparameters\n",
        "max_length = 30\n",
        "batch_size = 16\n",
        "num_epochs = 5\n",
        "learning_rate = 5e-5\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load captions\n",
        "with open(captions_json, 'r') as f:\n",
        "   captions_data = json.load(f)\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "tokenizer.pad_token = tokenizer.eos_token  # pad token set to eos token\n",
        "\n",
        "# Dataset\n",
        "class ImageCaptionDataset(Dataset):\n",
        "   def __init__(self, image_folder, samples, tokenizer, transform):\n",
        "       self.image_folder = image_folder\n",
        "       self.samples = samples  # list of (img_name, caption)\n",
        "       self.tokenizer = tokenizer\n",
        "       self.transform = transform\n",
        "\n",
        "   def __len__(self):\n",
        "       return len(self.samples)\n",
        "\n",
        "   def __getitem__(self, idx):\n",
        "       img_name, caption = self.samples[idx]\n",
        "       img_path = os.path.join(self.image_folder, img_name)\n",
        "\n",
        "       try:\n",
        "           image = Image.open(img_path).convert(\"RGB\")\n",
        "       except (OSError, UnidentifiedImageError):\n",
        "           # fallback black image if unreadable\n",
        "           image = Image.new('RGB', (224, 224), (0, 0, 0))\n",
        "\n",
        "       image = self.transform(image)\n",
        "       tokens = self.tokenizer(caption, return_tensors='pt', padding='max_length',\n",
        "                               truncation=True, max_length=max_length)\n",
        "       input_ids = tokens['input_ids'].squeeze(0)\n",
        "       attention_mask = tokens['attention_mask'].squeeze(0)\n",
        "\n",
        "       return image, input_ids, attention_mask, caption\n",
        "\n",
        "# Transforms for images\n",
        "transform = transforms.Compose([\n",
        "   transforms.Resize((224, 224)),\n",
        "   transforms.ToTensor(),\n",
        "   transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                        [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Prepare all samples (img_name, caption)\n",
        "all_samples = [(img, cap) for img, caps in captions_data.items() for cap in caps]\n",
        "\n",
        "# Train-validation split\n",
        "train_samples, val_samples = train_test_split(all_samples, test_size=0.1, random_state=42)\n",
        "\n",
        "# Create Datasets and DataLoaders\n",
        "train_dataset = ImageCaptionDataset(image_folder, train_samples, tokenizer, transform)\n",
        "val_dataset = ImageCaptionDataset(image_folder, val_samples, tokenizer, transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=18)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=18)\n",
        "\n",
        "# Encoder: EfficientNet-B0 without classifier head\n",
        "encoder_cnn = models.efficientnet_b0(pretrained=True)\n",
        "encoder_cnn.classifier = nn.Identity()\n",
        "encoder_cnn.to(device)\n",
        "encoder_cnn.eval()  # frozen encoder\n",
        "\n",
        "# Decoder: GPT-2 LM head model\n",
        "decoder = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "decoder.resize_token_embeddings(len(tokenizer))\n",
        "decoder.to(device)\n",
        "\n",
        "# Projector: project encoder features to GPT-2 embedding dimension\n",
        "projector = nn.Linear(1280, decoder.config.n_embd).to(device)\n",
        "\n",
        "# Optimizer and Scheduler\n",
        "optimizer = torch.optim.AdamW(list(projector.parameters()) + list(decoder.parameters()), lr=learning_rate)\n",
        "lr_scheduler = get_scheduler(\n",
        "   \"linear\",\n",
        "   optimizer=optimizer,\n",
        "   num_warmup_steps=0,\n",
        "   num_training_steps=len(train_loader) * num_epochs\n",
        ")\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "\n",
        "# Training loop\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "   decoder.train()\n",
        "   projector.train()\n",
        "   total_train_loss = 0.0\n",
        "\n",
        "   loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\")\n",
        "   for images, input_ids, attention_mask, _ in loop:\n",
        "       images, input_ids, attention_mask = images.to(device), input_ids.to(device), attention_mask.to(device)\n",
        "\n",
        "       with torch.no_grad():\n",
        "           image_features = encoder_cnn(images)\n",
        "\n",
        "       image_embeddings = projector(image_features).unsqueeze(1)  # (batch, 1, embd_dim)\n",
        "\n",
        "       # Get embeddings for captions\n",
        "       caption_embeddings = decoder.transformer.wte(input_ids)  # (batch, seq_len, embd_dim)\n",
        "\n",
        "       # Concatenate image embedding + caption embeddings along sequence dim\n",
        "       decoder_inputs = torch.cat([image_embeddings, caption_embeddings], dim=1)\n",
        "\n",
        "       # Labels: input_ids + pad at end (shifted right)\n",
        "       labels = torch.cat([input_ids, torch.full((input_ids.shape[0], 1), tokenizer.pad_token_id, device=device)], dim=1)\n",
        "\n",
        "       outputs = decoder(inputs_embeds=decoder_inputs)\n",
        "       logits = outputs.logits[:, :-1, :].contiguous()  # align logits and labels\n",
        "\n",
        "       loss = loss_fn(logits.view(-1, logits.size(-1)), labels[:, :-1].contiguous().view(-1))\n",
        "\n",
        "       loss.backward()\n",
        "       optimizer.step()\n",
        "       lr_scheduler.step()\n",
        "       optimizer.zero_grad()\n",
        "\n",
        "       total_train_loss += loss.item()\n",
        "       loop.set_postfix(loss=loss.item())\n",
        "\n",
        "   avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "   # Validation\n",
        "   decoder.eval()\n",
        "   projector.eval()\n",
        "   total_val_loss = 0.0\n",
        "   with torch.no_grad():\n",
        "       for images, input_ids, attention_mask, _ in val_loader:\n",
        "           images, input_ids = images.to(device), input_ids.to(device)\n",
        "\n",
        "           image_features = encoder_cnn(images)\n",
        "           image_embeddings = projector(image_features).unsqueeze(1)\n",
        "\n",
        "           caption_embeddings = decoder.transformer.wte(input_ids)\n",
        "\n",
        "           decoder_inputs = torch.cat([image_embeddings, caption_embeddings], dim=1)\n",
        "\n",
        "           labels = torch.cat([input_ids, torch.full((input_ids.shape[0], 1), tokenizer.pad_token_id, device=device)], dim=1)\n",
        "\n",
        "           outputs = decoder(inputs_embeds=decoder_inputs)\n",
        "           logits = outputs.logits[:, :-1, :].contiguous()\n",
        "\n",
        "           val_loss = loss_fn(logits.view(-1, logits.size(-1)), labels[:, :-1].contiguous().view(-1))\n",
        "           total_val_loss += val_loss.item()\n",
        "\n",
        "   avg_val_loss = total_val_loss / len(val_loader)\n",
        "   print(f\"\\nEpoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "   # Save best model\n",
        "   if avg_val_loss < best_val_loss:\n",
        "       best_val_loss = avg_val_loss\n",
        "       torch.save({\n",
        "           'projector_state_dict': projector.state_dict(),\n",
        "           'decoder_state_dict': decoder.state_dict(),\n",
        "           'tokenizer': tokenizer,\n",
        "           'epoch': epoch + 1\n",
        "       }, os.path.join(model_save_path, f'best_caption_model_epoch{epoch+1}.pt'))\n",
        "       print(\"âœ… Best model saved.\\n\")\n",
        "\n",
        "   # Generate sample caption (beam search) from first val image\n",
        "   sample_img, _, _, _ = val_dataset[0]\n",
        "   sample_img = sample_img.unsqueeze(0).to(device)\n",
        "\n",
        "   with torch.no_grad():\n",
        "       image_features = encoder_cnn(sample_img)\n",
        "       image_embedding = projector(image_features).unsqueeze(1)\n",
        "\n",
        "   output = decoder.generate(\n",
        "       inputs_embeds=torch.cat([image_embedding, decoder.transformer.wte(torch.full((1, 1), tokenizer.bos_token_id, dtype=torch.long).to(device))], dim=1),\n",
        "       max_length=max_length,\n",
        "       num_beams=5,\n",
        "       early_stopping=True,\n",
        "       bos_token_id=tokenizer.bos_token_id,\n",
        "       eos_token_id=tokenizer.eos_token_id\n",
        "   )\n",
        "   caption = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "   print(\"ðŸ–¼ï¸ Sample Generated Caption (Beam Search):\", caption)\n",
        "\n",
        "# Final BLEU evaluation on 100 validation images\n",
        "decoder.eval()\n",
        "y_true, y_pred = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "   for i in range(min(100, len(val_dataset))):\n",
        "       image, _, _, ref_caption = val_dataset[i]\n",
        "       image = image.unsqueeze(0).to(device)\n",
        "\n",
        "       image_features = encoder_cnn(image)\n",
        "       image_embedding = projector(image_features).unsqueeze(1)\n",
        "\n",
        "       output = decoder.generate(\n",
        "           inputs_embeds=torch.cat([image_embedding, decoder.transformer.wte(torch.full((1, 1), tokenizer.bos_token_id, dtype=torch.long).to(device))], dim=1),\n",
        "           max_length=max_length,\n",
        "           num_beams=5,\n",
        "           early_stopping=True,\n",
        "           bos_token_id=tokenizer.bos_token_id,\n",
        "           eos_token_id=tokenizer.eos_token_id\n",
        "       )\n",
        "       pred_caption = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "       y_pred.append(pred_caption.split())\n",
        "       y_true.append([ref_caption.split()])\n",
        "\n",
        "bleu = corpus_bleu(y_true, y_pred)\n",
        "print(f\"\\nðŸ“Š Final BLEU Score (Beam Search): {bleu:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine tuning"
      ],
      "metadata": {
        "id": "WrOKN7Mz5vIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import random\n",
        "import pandas as pd\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, get_scheduler\n",
        "from tqdm import tqdm\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# --- Paths ---\n",
        "image_folder = '/content/MyDrive/MyDrive/CV/Images/flickr30k_images'\n",
        "captions_json = '/content/MyDrive/MyDrive/CV/flickr_captions.json'\n",
        "\n",
        "# Path to load the previously trained best model\n",
        "original_model_load_path = '/content/MyDrive/MyDrive/best_full_caption_model/best_caption_model_epoch4.pt'\n",
        "\n",
        "# DIRECTLY assign the path to your pre-existing fine-tuned models folder\n",
        "finetuned_model_save_folder = '/content/MyDrive/MyDrive/best_full_caption_model/fine_tuned_best_model/' # <--- Make sure this path exists in your Drive!\n",
        "os.makedirs(finetuned_model_save_folder, exist_ok=True) # Ensure the folder exists (good practice even if you created it)\n",
        "\n",
        "\n",
        "# Hyperparameters (Consider adjusting for fine-tuning)\n",
        "max_length = 30\n",
        "batch_size = 16\n",
        "num_epochs_finetune = 5 # You can increase this to train for more epochs\n",
        "learning_rate_finetune = 2e-5 # Often good to reduce learning rate for fine-tuning\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# --- Load captions, Tokenizer, Dataset, Dataloaders (Keep as is) ---\n",
        "with open(captions_json, 'r') as f:\n",
        "    captions_data = json.load(f)\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "class ImageCaptionDataset(Dataset):\n",
        "    def __init__(self, image_folder, samples, tokenizer, transform):\n",
        "        self.image_folder = image_folder\n",
        "        self.samples = samples\n",
        "        self.tokenizer = tokenizer\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name, caption = self.samples[idx]\n",
        "        img_path = os.path.join(self.image_folder, img_name)\n",
        "        try:\n",
        "            image = Image.open(img_path).convert(\"RGB\")\n",
        "        except (OSError, UnidentifiedImageError):\n",
        "            image = Image.new('RGB', (224, 224), (0, 0, 0))\n",
        "        image = self.transform(image)\n",
        "        tokens = self.tokenizer(caption, return_tensors='pt', padding='max_length',\n",
        "                                 truncation=True, max_length=max_length)\n",
        "        input_ids = tokens['input_ids'].squeeze(0)\n",
        "        attention_mask = tokens['attention_mask'].squeeze(0)\n",
        "        return image, input_ids, attention_mask, caption\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "all_samples = [(img, cap) for img, caps in captions_data.items() for cap in caps]\n",
        "train_samples, val_samples = train_test_split(all_samples, test_size=0.1, random_state=42)\n",
        "\n",
        "train_dataset = ImageCaptionDataset(image_folder, train_samples, tokenizer, transform)\n",
        "val_dataset = ImageCaptionDataset(image_folder, val_samples, tokenizer, transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=18)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=18)\n",
        "\n",
        "# --- Encoder (Keep as is, frozen) ---\n",
        "encoder_cnn = models.efficientnet_b0(pretrained=True)\n",
        "encoder_cnn.classifier = nn.Identity()\n",
        "encoder_cnn.to(device)\n",
        "encoder_cnn.eval()\n",
        "\n",
        "# --- Load pre-trained Decoder and Projector weights ---\n",
        "decoder = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "decoder.resize_token_embeddings(len(tokenizer))\n",
        "projector = nn.Linear(1280, decoder.config.n_embd)\n",
        "\n",
        "# Load the best performing checkpoint (Epoch 4 from your previous run)\n",
        "print(f\" Loading model from: {original_model_load_path}\")\n",
        "checkpoint = torch.load(original_model_load_path, map_location=device, weights_only=False)\n",
        "decoder.load_state_dict(checkpoint['decoder_state_dict'])\n",
        "projector.load_state_dict(checkpoint['projector_state_dict'])\n",
        "\n",
        "decoder.to(device)\n",
        "projector.to(device)\n",
        "\n",
        "# --- Optimizer and Scheduler (Re-initialize for fine-tuning) ---\n",
        "optimizer = torch.optim.AdamW(list(projector.parameters()) + list(decoder.parameters()), lr=learning_rate_finetune)\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=len(train_loader) * num_epochs_finetune\n",
        ")\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "\n",
        "# --- Evaluation Function ---\n",
        "def evaluate_model(encoder, decoder, projector, data_loader, tokenizer, device, max_length):\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "    projector.eval()\n",
        "\n",
        "    generated_captions = []\n",
        "    reference_captions = []\n",
        "\n",
        "    print(\"\\nStarting evaluation...\")\n",
        "    with torch.no_grad():\n",
        "        for images, _, _, original_captions in tqdm(data_loader, desc=\"Generating captions for evaluation\"):\n",
        "            images = images.to(device)\n",
        "\n",
        "            image_features = encoder(images)\n",
        "            image_embeddings = projector(image_features).unsqueeze(1)\n",
        "\n",
        "            # Generate captions using beam search for better quality\n",
        "            outputs = decoder.generate(\n",
        "                inputs_embeds=torch.cat([image_embeddings, decoder.transformer.wte(torch.full((images.shape[0], 1), tokenizer.bos_token_id, dtype=torch.long).to(device))], dim=1),\n",
        "                max_length=max_length,\n",
        "                num_beams=5,\n",
        "                early_stopping=True,\n",
        "                bos_token_id=tokenizer.bos_token_id,\n",
        "                eos_token_id=tokenizer.eos_token_id,\n",
        "                attention_mask=torch.ones(image_embeddings.shape[0], image_embeddings.shape[1] + 1, dtype=torch.long).to(device)\n",
        "            )\n",
        "\n",
        "            decoded_captions = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
        "            generated_captions.extend(decoded_captions)\n",
        "            reference_captions.extend([[cap] for cap in original_captions]) # corpus_bleu expects a list of lists for references\n",
        "\n",
        "    # Calculate BLEU score\n",
        "    # Ensure all references and hypotheses are tokenized\n",
        "    tokenized_generated_captions = [cap.split() for cap in generated_captions]\n",
        "    tokenized_reference_captions = [[cap.split()] for ref_list in reference_captions for cap in ref_list] # Flatten and tokenize\n",
        "\n",
        "    bleu_score = corpus_bleu(tokenized_reference_captions, tokenized_generated_captions)\n",
        "    return generated_captions, reference_captions, bleu_score\n",
        "\n",
        "# --- Training loop ---\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(num_epochs_finetune):\n",
        "    decoder.train()\n",
        "    projector.train()\n",
        "    total_train_loss = 0.0\n",
        "\n",
        "    loop = tqdm(train_loader, desc=f\"Fine-tune Epoch {epoch+1}/{num_epochs_finetune} - Training\")\n",
        "    for images, input_ids, attention_mask, _ in loop:\n",
        "        images, input_ids, attention_mask = images.to(device), input_ids.to(device), attention_mask.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            image_features = encoder_cnn(images)\n",
        "\n",
        "        image_embeddings = projector(image_features).unsqueeze(1)\n",
        "\n",
        "        caption_embeddings = decoder.transformer.wte(input_ids)\n",
        "\n",
        "        decoder_inputs = torch.cat([image_embeddings, caption_embeddings], dim=1)\n",
        "\n",
        "        labels = torch.cat([input_ids, torch.full((input_ids.shape[0], 1), tokenizer.pad_token_id, device=device)], dim=1)\n",
        "\n",
        "        outputs = decoder(inputs_embeds=decoder_inputs)\n",
        "        logits = outputs.logits[:, :-1, :].contiguous()\n",
        "\n",
        "        loss = loss_fn(logits.view(-1, logits.size(-1)), labels[:, :-1].contiguous().view(-1))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "        loop.set_postfix(loss=loss.item())\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "    # Validation\n",
        "    decoder.eval()\n",
        "    projector.eval()\n",
        "    total_val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for images, input_ids, attention_mask, _ in val_loader:\n",
        "            images, input_ids = images.to(device), input_ids.to(device)\n",
        "\n",
        "            image_features = encoder_cnn(images)\n",
        "            image_embeddings = projector(image_features).unsqueeze(1)\n",
        "\n",
        "            caption_embeddings = decoder.transformer.wte(input_ids)\n",
        "\n",
        "            decoder_inputs = torch.cat([image_embeddings, caption_embeddings], dim=1)\n",
        "\n",
        "            labels = torch.cat([input_ids, torch.full((input_ids.shape[0], 1), tokenizer.pad_token_id, device=device)], dim=1)\n",
        "\n",
        "            outputs = decoder(inputs_embeds=decoder_inputs)\n",
        "            logits = outputs.logits[:, :-1, :].contiguous()\n",
        "\n",
        "            val_loss = loss_fn(logits.view(-1, logits.size(-1)), labels[:, :-1].contiguous().view(-1))\n",
        "            total_val_loss += val_loss.item()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(val_loader)\n",
        "    print(f\"\\nFine-tune Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    # Save best model to the DIRECTLY assigned fine-tuned model folder\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        # Using the new path: finetuned_model_save_folder\n",
        "        torch.save({\n",
        "            'projector_state_dict': projector.state_dict(),\n",
        "            'decoder_state_dict': decoder.state_dict(),\n",
        "            'tokenizer': tokenizer,\n",
        "            'epoch': checkpoint['epoch'] + epoch + 1,\n",
        "            'best_val_loss': best_val_loss\n",
        "        }, os.path.join(finetuned_model_save_folder, f'best_caption_model_finetuned_epoch{checkpoint[\"epoch\"] + epoch + 1}.pt'))\n",
        "        print(f\" Best fine-tuned model saved to {finetuned_model_save_folder}.\\n\")\n",
        "\n",
        "    # Generate sample caption (beam search) from first val image\n",
        "    # Note: To avoid issues with random access for `val_dataset[0]` if `num_workers` > 0\n",
        "    # it's safer to get a sample from the first batch of the DataLoader.\n",
        "    # For a quick sample, let's keep it for now but be aware of potential issues in a highly parallel setup.\n",
        "    sample_img_tensor, _, _, sample_original_caption = val_dataset[0] # Get image tensor and its original caption\n",
        "    sample_img_tensor = sample_img_tensor.unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        image_features = encoder_cnn(sample_img_tensor)\n",
        "        image_embedding = projector(image_features).unsqueeze(1)\n",
        "\n",
        "    output = decoder.generate(\n",
        "        inputs_embeds=torch.cat([image_embedding, decoder.transformer.wte(torch.full((1, 1), tokenizer.bos_token_id, dtype=torch.long).to(device))], dim=1),\n",
        "        max_length=max_length,\n",
        "        num_beams=5,\n",
        "        early_stopping=True,\n",
        "        bos_token_id=tokenizer.bos_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        attention_mask=torch.ones(image_embedding.shape[0], image_embedding.shape[1] + 1, dtype=torch.long).to(device)\n",
        "    )\n",
        "    generated_sample_caption = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    print(f\" Sample Image Original Caption: {sample_original_caption}\")\n",
        "    print(f\" Sample Generated Caption (Beam Search) (Fine-tuned): {generated_sample_caption}\\n\")\n",
        "\n",
        "\n",
        "# --- Final Evaluation after Fine-tuning ---\n",
        "print(\"\\n--- Running Final Evaluation ---\")\n",
        "all_generated_captions, all_reference_captions, final_bleu_score = evaluate_model(\n",
        "    encoder_cnn, decoder, projector, val_loader, tokenizer, device, max_length\n",
        ")\n",
        "\n",
        "print(f\"Final BLEU Score: {final_bleu_score * 100:.2f}%\")\n",
        "\n",
        "print(\"\\n--- Fine-tuning Complete ---\")\n"
      ],
      "metadata": {
        "id": "QVtdN1Ak5naU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}